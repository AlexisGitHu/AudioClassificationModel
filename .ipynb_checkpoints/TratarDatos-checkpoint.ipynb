{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d5ff28-5ddc-4ece-b82c-a4c33a42ca3f",
   "metadata": {},
   "source": [
    "# Tratar datos\n",
    "\n",
    "Tratar los datos ajustándolos al formato legible por el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf9dd3f-5a6e-470e-87b5-995eba6e3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4824b3-0b61-4243-8510-21329ee731c0",
   "metadata": {},
   "source": [
    "# ESPECTROGRAMA NUEVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1288fc96-1ed1-453c-ac30-ede16e5aef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "#from nsynth import NSynth, SignalTransformation\n",
    "import sys\n",
    "import librosa.display\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "638229a9-fdfa-4305-a83f-111b6f7f27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSynth(data.Dataset):\n",
    "\n",
    "    \"\"\"Pytorch dataset for NSynth dataset\n",
    "    args:\n",
    "        root: root dir containing examples.json and audio directory with\n",
    "            wav files.\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "                a sample and returns a transformed version.\n",
    "        target_transform (callable, optional): A function/transform that takes\n",
    "            in the target and transforms it.\n",
    "        blacklist_pattern: list of string used to blacklist dataset element.\n",
    "            If one of the string is present in the audio filename, this sample\n",
    "            together with its metadata is removed from the dataset.\n",
    "        categorical_field_list: list of string. Each string is a key like\n",
    "            instrument_family that will be used as a classification target.\n",
    "            Each field value will be encoding as an integer using sklearn\n",
    "            LabelEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 blacklist_pattern=[],\n",
    "                 categorical_field_list=[\"instrument_family\"]):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        assert(isinstance(root, str))\n",
    "        assert(isinstance(blacklist_pattern, list))\n",
    "        assert(isinstance(categorical_field_list, list))\n",
    "        self.root = root\n",
    "        self.filenames = glob.glob(os.path.join(root, \"audio/*.wav\"))\n",
    "        with open(os.path.join(root, \"examples.json\"), \"r\") as f:\n",
    "            self.json_data = json.load(f)\n",
    "        for pattern in blacklist_pattern:\n",
    "            self.filenames, self.json_data = self.blacklist(\n",
    "                self.filenames, self.json_data, pattern)\n",
    "        self.categorical_field_list = categorical_field_list\n",
    "        self.le = []\n",
    "        for i, field in enumerate(self.categorical_field_list):\n",
    "            self.le.append(LabelEncoder())\n",
    "            field_values = [value[field] for value in self.json_data.values()]\n",
    "            self.le[i].fit(field_values)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def blacklist(self, filenames, json_data, pattern):\n",
    "        filenames = [filename for filename in filenames\n",
    "                     if pattern not in filename]\n",
    "        json_data = {\n",
    "            key: value for key, value in json_data.items()\n",
    "            if pattern not in key\n",
    "        }\n",
    "        return filenames, json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (audio sample, *categorical targets, json_data)\n",
    "        \"\"\"\n",
    "        name = self.filenames[index]\n",
    "        _, sample = scipy.io.wavfile.read(name)\n",
    "        target = self.json_data[os.path.splitext(os.path.basename(name))[0]]\n",
    "        categorical_target = [\n",
    "            le.transform([target[field]])[0]\n",
    "            for field, le in zip(self.categorical_field_list, self.le)]\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return [sample, *categorical_target, target]\n",
    "\n",
    "    def getAudioCharacteristics(self, index):\n",
    "        item = self.__getitem__(index)\n",
    "        return {\"fichero\": item[3][\"note_str\"]+\".wav\",\"familia\":item[3][\"instrument_source\"]}\n",
    "\n",
    "    # def getSignal(self,index):\n",
    "    #     #La ruta va desde el fichero donde se encuentra el jupyter notebook\n",
    "    #     return torchaudio.load(\"nsynth-valid/audio/\"+self.getAudioCharacteristics(index)[\"fichero\"])\n",
    "\n",
    "    # def getLabel(self,index):\n",
    "    #     return self.getAudioCharacteristics(index)[\"familia\"]\n",
    "\n",
    "    # def samplingTransform(self,signal,sample_rate):\n",
    "    #     #Usamos sample_rate 16000 por defecto\n",
    "    #     if sample_rate != self.TARGET_SAMPLE_RATE:\n",
    "    #         signal = torchaudio.transforms.signalResample(sample_rate,self.TARGET_SAMPLE_RATE)(signal)\n",
    "    #     return signal\n",
    "            \n",
    "\n",
    "    # def monoTransform(self,signal):\n",
    "    #     if signal.shape[0] > 1:\n",
    "    #         signal = torch.mean(signal, dim=0,keepdim=True)\n",
    "    #     return signal\n",
    "    \n",
    "    # def setTargetSampleRate(self,sample_rate):\n",
    "    #     self.TARGET_SAMPLE_RATE=sample_rate\n",
    "    #     return 0\n",
    "\n",
    "    # def setTransformation(self,transformation):\n",
    "    #     self.TRANSFORMATION=transformation\n",
    "    #     return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdbf45d1-492f-4973-b918-1b1ac74a43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalTransformation():\n",
    "\n",
    "     #Sample rate por defecto\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    TARGET_SAMPLE_RATE=16000\n",
    "    #Usamos espectogramas de MEL por defecto al utilizar instrumentos musicales para captar las frecuencias melódicas de forma entendible para humanos y para el modelo\n",
    "    TRANSFORMATION=torchaudio.transforms.MelSpectrogram(sample_rate=TARGET_SAMPLE_RATE,\n",
    "                                                      n_fft=1024, \n",
    "                                                      hop_length=512,\n",
    "                                                      n_mels=64).to(device)\n",
    "\n",
    "    def __init__(self, fichero, label):\n",
    "        self.fichero=fichero\n",
    "        self.label=label\n",
    "    \n",
    "    def generarSpectrograma(self):\n",
    "        signal = self.TRANSFORMATION(self.getSignalTuned())\n",
    "        return signal\n",
    "\n",
    "    @classmethod\n",
    "    def generarSpectrogramaFromSignal(cls, signal):\n",
    "        signal = cls.monoTransformClass(signal)\n",
    "        signal = cls.samplingTransformClass(signal,cls.TARGET_SAMPLE_RATE)\n",
    "        signal = cls.TRANSFORMATION(signal)\n",
    "        return signal\n",
    "\n",
    "    @classmethod\n",
    "    def generarSTFTFromSignal(cls,signal):\n",
    "        signal = cls.monoTransformClass(signal)\n",
    "        signal = cls.samplingTransformClass(signal,cls.TARGET_SAMPLE_RATE)\n",
    "        signal = torchaudio.transforms.Spectrogram().to(cls.device)(signal)\n",
    "        return signal\n",
    "    def getSignal(self):\n",
    "        #La ruta va desde el fichero donde se encuentra el jupyter notebook\n",
    "        return torchaudio.load(\"nsynth-valid/audio/\"+self.fichero)\n",
    "\n",
    "    def getSignalTuned(self):\n",
    "        signal, sr  = self.getSignal()\n",
    "        signal= self.monoTransform(signal)\n",
    "        signal = self.samplingTransform(signal,self.TARGET_SAMPLE_RATE)\n",
    "        return signal \n",
    "\n",
    "    def getLabel(self):\n",
    "        return self.label\n",
    "    def getSignalAndLabel(self):\n",
    "        return self.getSignal(), self.getLabel()\n",
    "\n",
    "    def samplingTransform(self,signal,sample_rate):\n",
    "        #Usamos sample_rate 16000 por defecto\n",
    "        if sample_rate != self.TARGET_SAMPLE_RATE:\n",
    "            signal = torchaudio.transforms.signalResample(sample_rate,self.TARGET_SAMPLE_RATE)(signal)\n",
    "        return signal\n",
    "\n",
    "    @classmethod\n",
    "    def samplingTransformClass(cls,signal,sample_rate):\n",
    "        #Usamos sample_rate 16000 por defecto\n",
    "        if sample_rate != cls.TARGET_SAMPLE_RATE:\n",
    "            signal = torchaudio.transforms.signalResample(sample_rate,cls.TARGET_SAMPLE_RATE)(signal)\n",
    "        return signal\n",
    "            \n",
    "    @classmethod        \n",
    "    def monoTransformClass(cls, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0,keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def monoTransform(self,signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0,keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def setTargetSampleRate(self,sample_rate):\n",
    "        self.TARGET_SAMPLE_RATE=sample_rate\n",
    "        return 0\n",
    "\n",
    "    def setTransformation(self,transformation):\n",
    "        self.TRANSFORMATION=transformation\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fdce9c9-e09a-400c-bdfc-3f16b85d0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f4b1488-bed9-4574-8666-39cac76a0cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452608\n",
      "64000\n",
      "64000\n",
      "64000\n",
      "64000\n",
      "64000\n",
      "64000\n",
      "64000\n"
     ]
    }
   ],
   "source": [
    "def separar(audio):\n",
    "    waveform, sample_rate = torchaudio.load(audio)\n",
    "    dimension = waveform.size(dim=1)\n",
    "    print(dimension)\n",
    "    #prueba = torch.narrow(waveform, dim=1, start =0, length = 64000)\n",
    "    #print(prueba.size(dim=1))\n",
    "    #plot_specgram_prueba(prueba,sample_rate,audio,0)\n",
    "    #plot_specgram_prueba(prueba,sample_rate,audio,i)\n",
    "    anterior = 1\n",
    "    i=0\n",
    "    k=32000\n",
    "    while (anterior+64000) <= dimension:\n",
    "            prueba = torch.narrow(waveform, dim=1, start = anterior, length = 64000)\n",
    "            anterior = anterior + k\n",
    "            print(prueba.size(dim=1))\n",
    "            #plot_specgram_prueba(prueba,sample_rate,audio,i)\n",
    "            prueba = SignalTransformation.generarSpectrogramaFromSignal(prueba)\n",
    "            shape = prueba.shape\n",
    "            prueba = torchaudio.transforms.AmplitudeToDB()(prueba) \n",
    "            prueba=prueba.cpu().data.numpy()\n",
    "            librosa.display.specshow(prueba[0],cmap='magma')\n",
    "            nombre = \"./nsynth-test/guitarra6/{}.png\".format(i)\n",
    "            plt.savefig(nombre, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            i = i+1\n",
    "    #plot_specgram_prueba(waveform,sample_rate,audio)\n",
    "\n",
    "    \n",
    "#separar(\"nsynth-test/audio/bass_electronic_018-026-025.wav\")\n",
    "#separar(\"nsynth-test/Prueba-1.wav\")\n",
    "#separar(\"nsynth-test/Prueba-2.wav\")\n",
    "separar(\"nsynth-test/Guitarra6.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16555c71-0b05-487f-9fae-21b626c9a5ef",
   "metadata": {},
   "source": [
    "Crear el espectograma entero de la muestra para comprobar que sirven "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab3c40ee-4528-47b3-983d-41fd03712e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba, sample_rate = torchaudio.load(\"nsynth-test/Guitarra6.wav\")\n",
    "prueba = SignalTransformation.generarSpectrogramaFromSignal(prueba)\n",
    "shape = prueba.shape\n",
    "prueba = torchaudio.transforms.AmplitudeToDB()(prueba) \n",
    "prueba=prueba.cpu().data.numpy()\n",
    "librosa.display.specshow(prueba[0],cmap='magma')\n",
    "i=1000\n",
    "nombre = \"./nsynth-test/guitarra6/{}.png\".format(i)\n",
    "plt.savefig(nombre, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
